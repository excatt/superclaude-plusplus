---
name: eval-harness
description: AI ê°œë°œì„ ìœ„í•œ í‰ê°€ ê¸°ë°˜ ê°œë°œ(EDD) í”„ë ˆì„ì›Œí¬. AI ê¸°ëŠ¥ êµ¬í˜„, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, LLM í†µí•© ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤. Keywords: eval, evaluation, test, AI, LLM, prompt, benchmark, quality, EDD, í‰ê°€, ë²¤ì¹˜ë§ˆí¬.
---

# Eval Harness Skill (Eval-Driven Development)

## Purpose
AI ê¸°ëŠ¥ ê°œë°œ ì‹œ "í‰ê°€ë¥¼ ë¨¼ì € ì •ì˜í•˜ê³ , ê·¸ í‰ê°€ë¥¼ í†µê³¼í•˜ë„ë¡ êµ¬í˜„"í•˜ëŠ” EDD(Eval-Driven Development) ë°©ë²•ë¡ ì„ ì ìš©í•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**: Evalì€ AI ê°œë°œì˜ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ â†’ ì„±ê³µ ê¸°ì¤€ ë¨¼ì € ì •ì˜ â†’ êµ¬í˜„ â†’ í‰ê°€ â†’ ë°˜ë³µ

## Activation Triggers
- AI/LLM ê¸°ëŠ¥ êµ¬í˜„ ì‹œ
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…
- AI í’ˆì§ˆ ë²¤ì¹˜ë§ˆí¬ í•„ìš” ì‹œ
- íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì • ì‹œ
- ì‚¬ìš©ì ëª…ì‹œì  ìš”ì²­: `/eval`, `í‰ê°€ ì„¤ì •`, `ë²¤ì¹˜ë§ˆí¬`

---

## Core Concepts

### Eval = Unit Test for AI
```
Traditional Dev          AI Dev (EDD)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Unit Test                Eval
Expected Output          Success Criteria
assert(result == x)      score >= threshold
Deterministic            Probabilistic
100% pass required       pass@k metric
```

### Three Eval Types

| Type | Purpose | Example |
|------|---------|---------|
| **Capability Eval** | ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ | "ìš”ì•½ ê¸°ëŠ¥ì´ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í¬í•¨í•˜ëŠ”ê°€?" |
| **Regression Eval** | ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ | "ê¸°ì¡´ ë¶„ë¥˜ ì •í™•ë„ê°€ ìœ ì§€ë˜ëŠ”ê°€?" |
| **Safety Eval** | ì•ˆì „ì„± ê²€ì¦ | "ìœ í•´ ì½˜í…ì¸ ë¥¼ ê±°ë¶€í•˜ëŠ”ê°€?" |

---

## EDD Workflow

### Phase 1: Define (í‰ê°€ ì •ì˜)
```
/eval define <feature-name>

ğŸ“ Eval Definition
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Feature: document-summarizer
Type: Capability

Success Criteria:
â–¡ ìš”ì•½ì´ ì›ë¬¸ì˜ 30% ì´í•˜ ê¸¸ì´
â–¡ í•µì‹¬ í‚¤ì›Œë“œ 80% ì´ìƒ í¬í•¨
â–¡ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë¬¸ì¥
â–¡ í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ìŒ

Test Cases:
1. ì§§ì€ ë‰´ìŠ¤ ê¸°ì‚¬ (200ë‹¨ì–´)
2. ê¸´ ê¸°ìˆ  ë¬¸ì„œ (2000ë‹¨ì–´)
3. êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ (í‘œ í¬í•¨)
4. ë‹¤êµ­ì–´ ë¬¸ì„œ (ì˜/í•œ í˜¼í•©)

Grading Method: Model-based + Code-based hybrid
Target: pass@3 > 90%
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Phase 2: Implement (êµ¬í˜„)
í‰ê°€ ì •ì˜ì— ë§ì¶° ê¸°ëŠ¥ êµ¬í˜„

### Phase 3: Evaluate (í‰ê°€ ì‹¤í–‰)
```
/eval run <feature-name>

ğŸ§ª Running Evals...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Feature: document-summarizer
Test Cases: 4
Trials per case: 3

Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Case       â”‚ Trial 1 â”‚ Trial 2 â”‚ Trial 3 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Short news      â”‚ âœ… PASS â”‚ âœ… PASS â”‚ âœ… PASS â”‚
â”‚ Long tech doc   â”‚ âœ… PASS â”‚ âŒ FAIL â”‚ âœ… PASS â”‚
â”‚ Structured      â”‚ âœ… PASS â”‚ âœ… PASS â”‚ âœ… PASS â”‚
â”‚ Multilingual    â”‚ âŒ FAIL â”‚ âœ… PASS â”‚ âœ… PASS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Metrics:
- pass@1: 75% (3/4)
- pass@3: 100% (4/4) âœ…
- Average score: 0.87

Status: âœ… PASSED (pass@3 > 90%)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Phase 4: Report (ë³´ê³ ì„œ)
```
/eval report

ğŸ“Š Eval Report: document-summarizer
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Version: 1.0.0
Date: 2025-01-26
Baseline: v0.9.0

Performance:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric         â”‚ Target  â”‚ Actual  â”‚ Status â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ pass@3         â”‚ > 90%   â”‚ 100%    â”‚ âœ…     â”‚
â”‚ Avg Score      â”‚ > 0.8   â”‚ 0.87    â”‚ âœ…     â”‚
â”‚ Latency (p50)  â”‚ < 2s    â”‚ 1.2s    â”‚ âœ…     â”‚
â”‚ Latency (p99)  â”‚ < 5s    â”‚ 3.8s    â”‚ âœ…     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Regression Check:
- Classification: âœ… No regression
- Entity extraction: âœ… No regression
- Sentiment: âš ï¸ -2% (within tolerance)

Recommendation: Ready for production
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Grading Methods

### Code-Based Graders (Deterministic)
```python
# ê¸¸ì´ ê²€ì¦
def check_length(summary, original):
    return len(summary) < len(original) * 0.3

# í‚¤ì›Œë“œ í¬í•¨ ê²€ì¦
def check_keywords(summary, keywords):
    found = sum(1 for k in keywords if k in summary)
    return found / len(keywords) >= 0.8

# ë¹Œë“œ ì„±ê³µ ê²€ì¦
def check_build():
    return subprocess.run(["npm", "run", "build"]).returncode == 0
```

### Model-Based Graders (LLM Judge)
```python
JUDGE_PROMPT = """
ë‹¤ìŒ ìš”ì•½ì„ í‰ê°€í•˜ì„¸ìš”:

ì›ë¬¸: {original}
ìš”ì•½: {summary}

í‰ê°€ ê¸°ì¤€:
1. í•µì‹¬ ì •ë³´ í¬í•¨ (1-5)
2. ê°„ê²°ì„± (1-5)
3. ì •í™•ì„± (1-5)
4. ê°€ë…ì„± (1-5)

JSON í˜•ì‹ìœ¼ë¡œ ì ìˆ˜ì™€ ì´ìœ ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""

def model_grade(original, summary):
    response = llm.generate(JUDGE_PROMPT.format(...))
    scores = json.loads(response)
    return sum(scores.values()) / 20  # 0-1 ì •ê·œí™”
```

### Human Graders (Manual Review)
```yaml
human_review:
  required_for:
    - safety_critical_decisions
    - edge_cases
    - low_confidence_results

  interface:
    - show: [input, output, criteria]
    - collect: [pass/fail, score, comments]
```

---

## Metrics

### pass@k
```
pass@k: kë²ˆ ì‹œë„ ì¤‘ ìµœì†Œ 1ë²ˆ ì„±ê³µ

pass@1 = ë‹¨ì¼ ì‹œë„ ì„±ê³µë¥  (ì—„ê²©)
pass@3 = 3ë²ˆ ì¤‘ 1ë²ˆ ì´ìƒ ì„±ê³µ (ì¼ë°˜ì )
pass@5 = 5ë²ˆ ì¤‘ 1ë²ˆ ì´ìƒ ì„±ê³µ (ê´€ëŒ€)
```

### pass^k (Critical)
```
pass^k: kë²ˆ ëª¨ë‘ ì„±ê³µí•´ì•¼ í†µê³¼

ì•ˆì „ ê´€ë ¨ ê¸°ëŠ¥ì— ì‚¬ìš©:
- ìœ í•´ ì½˜í…ì¸  í•„í„°ë§
- PII ê°ì§€
- ë³´ì•ˆ ê²€ì¦
```

### Score Threshold
```
score >= threshold

ì—°ì† ì ìˆ˜ í‰ê°€:
- í’ˆì§ˆ ì ìˆ˜
- ìœ ì‚¬ë„ ì ìˆ˜
- ì‹ ë¢°ë„ ì ìˆ˜
```

---

## Eval File Structure

### ì €ì¥ ìœ„ì¹˜
```
.claude/evals/
â”œâ”€â”€ capability/
â”‚   â”œâ”€â”€ document-summarizer.eval.md
â”‚   â””â”€â”€ code-generator.eval.md
â”œâ”€â”€ regression/
â”‚   â”œâ”€â”€ classification.eval.md
â”‚   â””â”€â”€ extraction.eval.md
â”œâ”€â”€ safety/
â”‚   â””â”€â”€ content-filter.eval.md
â””â”€â”€ baselines/
    â””â”€â”€ v1.0.0.json
```

### Eval Definition Format
```markdown
---
name: document-summarizer
type: capability
version: 1.0.0
created: 2025-01-26
---

# Document Summarizer Eval

## Success Criteria
- [ ] Summary length < 30% of original
- [ ] Contains 80%+ key concepts
- [ ] Grammatically correct
- [ ] No hallucinations

## Test Cases

### Case 1: Short News Article
**Input**: [news_article.txt]
**Expected**: Summary with main event, actors, outcome
**Grader**: model-based

### Case 2: Technical Documentation
**Input**: [tech_doc.md]
**Expected**: Summary with key concepts, no code
**Grader**: hybrid (code + model)

## Grading Configuration
```yaml
method: hybrid
code_checks:
  - length_ratio: 0.3
  - keyword_coverage: 0.8
model_judge:
  criteria: [accuracy, completeness, clarity]
  threshold: 0.8
```

## Targets
- pass@3: > 90%
- average_score: > 0.8
- latency_p99: < 5s
```

---

## Integration

### With `/verify`
```
/verify â†’ ì¼ë°˜ ì½”ë“œ í’ˆì§ˆ
/eval â†’ AI ê¸°ëŠ¥ í’ˆì§ˆ

ë‘˜ ë‹¤ í†µê³¼í•´ì•¼ PR Ready
```

### With `/feature-planner`
```
AI ê¸°ëŠ¥ Phase êµ¬ì¡°:
1. Eval ì •ì˜ (RED)
2. êµ¬í˜„ (GREEN)
3. í‰ê°€ ì‹¤í–‰
4. ë¦¬íŒ©í† ë§ (BLUE)
5. íšŒê·€ í…ŒìŠ¤íŠ¸ ì¶”ê°€
```

### With CI/CD
```yaml
# .github/workflows/eval.yml
eval:
  runs-on: ubuntu-latest
  steps:
    - name: Run Evals
      run: claude eval run --all

    - name: Check Regression
      run: claude eval regression --baseline v1.0.0

    - name: Upload Report
      uses: actions/upload-artifact@v3
      with:
        name: eval-report
        path: .claude/evals/reports/
```

---

## Commands

| Command | Description |
|---------|-------------|
| `/eval define <name>` | ìƒˆ í‰ê°€ ì •ì˜ |
| `/eval run <name>` | í‰ê°€ ì‹¤í–‰ |
| `/eval run --all` | ëª¨ë“  í‰ê°€ ì‹¤í–‰ |
| `/eval report` | í‰ê°€ ë³´ê³ ì„œ ìƒì„± |
| `/eval regression` | íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ |
| `/eval baseline <version>` | ê¸°ì¤€ì„  ì €ì¥ |

---

## Best Practices

### Eval ì‘ì„± ì›ì¹™
1. **êµ¬ì²´ì  ì„±ê³µ ê¸°ì¤€**: ëª¨í˜¸í•œ ê¸°ì¤€ í”¼í•˜ê¸°
2. **ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: ì—£ì§€ ì¼€ì´ìŠ¤ í¬í•¨
3. **ì ì ˆí•œ Grader ì„ íƒ**: ê²°ì •ì  vs í™•ë¥ ì 
4. **í˜„ì‹¤ì  ëª©í‘œ**: pass@1 100%ëŠ” ë¹„í˜„ì‹¤ì 

### ì•ˆí‹°íŒ¨í„´
```
âŒ "ê²°ê³¼ê°€ ì¢‹ì•„ì•¼ í•¨" â†’ ì¸¡ì • ë¶ˆê°€
âŒ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ â†’ ê³¼ì í•© ìœ„í—˜
âŒ pass@1 > 99% ëª©í‘œ â†’ ë¹„í˜„ì‹¤ì 
âŒ ëª¨ë“  ê²ƒì„ Model-basedë¡œ â†’ ë¹„ìš© ë° ì¼ê´€ì„± ë¬¸ì œ
```
